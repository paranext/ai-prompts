# Creating Projects Port Review

## Executive Summary
- Phase 1–2 artifacts capture PT9 behavior well, but they rarely translate it into actionable PT10 plans; the results stop at cataloging rather than telling an implementer where code should live or how to prove it works in paranext-core.
- Golden masters and contracts conflict with the stated workflow: Level A features are supposed to skip captured outputs, yet the “golden masters” are narrative specs that cannot be executed, leaving Phase 3 without a trustworthy oracle.
- The specification leans on abstract `IProjectService` + `paratext.*` command names that do not exist in the repo, so agents still have to rediscover the actual `Paranext.DataProvider` services, `LocalParatextProjects`, and command registration patterns before coding.
- Artefacts are highly duplicative (task description, README, spec summary, GitHub issue, phase-status all restate the same tables), and none of them expose PT10 test/setup steps, so the signal-to-noise ratio is low for agents trying to move fast.

## Detailed Findings
### 1. Workflow design observations (AI-Porting-Workflow.md)
- The Phase 2 description says the `golden-master-gen` agent “capture[s] PT9 outputs” and that golden masters are generated by running PT9 and storing serialized outputs (.context/AI-Porting-Workflow.md:124-219). A few lines later the same section claims Level A features should *not* capture PT9 output and should instead write specifications (.context/AI-Porting-Workflow.md:235-242). This contradiction resulted in textual “golden masters” for Creating Projects that are not runnable or verifiable.
- The document relies on a future “Alignment Agent” in Phase 3 to map contracts to paranext-core patterns (.context/AI-Porting-Workflow.md:126-133) but never tells the Phase 1–2 agents what PT10 information they must gather for that hand-off. Without embedding PT10 paths/names early, Phase 3 still has to rediscover namespace, file, and command conventions from scratch.
- Quality gates require characterization tests, property tests, mutation tests, and visual review for *every* feature (G1–G10, .context/AI-Porting-Workflow.md:175-188) but the workflow doc never maps those gates to concrete repos/projects (`c-sharp-tests`, `extensions`, `pnpm` scripts). This makes it unclear how an agent proves that a gate is satisfied inside paranext-core.

### 2. Creating Projects artifacts (feature folder)
- The golden master “expected outputs” are descriptive prose with placeholders such as `"projectDirectory": "{ProjectsPath}/TestProj/"` and fabricated GUID examples rather than serialized PT9 outputs (.context/features/creating-projects/golden-masters/gm-002-standard-project-creation/expected-output.json:2-58). They cannot be dropped into a test, so they do not actually derisk Phase 3.
- Data contracts define everything under `namespace Paratext.Data.PT10.Projects`, expose an `IProjectService`, and prescribe `paratext.*` command names (.context/features/creating-projects/data-contracts.md:78-105,566-677 and README.md:243-260) even though paranext-core’s root namespace is `Paranext.DataProvider` and commands follow the `{extensionName}.{commandName}` convention (.context/standards/paranext-core-patterns.md:11-35,150-188). An implementer still has to rework every contract to match the actual repo before coding.
- PT10-specific infrastructure (`LocalParatextProjects`, `ParatextProjectDataProviderFactory`, `ProjectSettingsService`) is only mentioned once in the task description (.context/features/creating-projects/task-description.md:39-44) and is absent from the behavior catalog, boundary map, and contracts. There is no plan for how the new service plugs into `LocalParatextProjects` or how to initialize ParatextGlobals, even though those classes are the actual entry points (c-sharp/Projects/LocalParatextProjects.cs:1-124).
- All Phase 1 tables are replicated across four files—task-description (lines 7-27), README (lines 9-198), spec-summary (lines 3-110), phase-status (lines 5-120), plus the GitHub issue. Keeping this redundant content in sync will be difficult and makes it harder for agents to find the one authoritative place to update classification, scope, and acceptance criteria.
- Phase status proudly reports 75 scenarios, 6 golden masters, and 19 validation rules with 100% coverage (.context/features/creating-projects/phase-status.md:25-119), but it never ranks which of those artifacts unblock PT10 implementation. The absence of prioritization leaves agents with a wall of text rather than a focused plan.

### 3. Alignment with paranext-core structure
- The contracts never state where code should live inside `c-sharp/`. The repo already exposes `Paranext.DataProvider.Projects.LocalParatextProjects` and related tests (c-sharp/Projects/LocalParatextProjects.cs:1-124 and c-sharp-tests/Projects/FixtureSetup.cs:6-35), but no artifact tells agents to reuse or extend these. Instead, they are told to create a new `IProjectService`, which would be off-pattern and harder to integrate with existing PAPI data providers.
- Command names such as `paratext.createProject` conflict with the documented naming convention (`'extensionName.commandName'`) and existing command registrations (.context/standards/paranext-core-patterns.md:150-188). Without explicit guidance, agents are likely to invent another namespace, causing inconsistent manifests.
- Nothing in Phase 2 spells out the TypeScript entry point (which extension will surface project creation, what `context.registrations.add` call should look like, how to extend `papi-shared-types`). The contracts only show free functions, so the bridge from backend service to UI is still undefined.

### 4. Making Phase 3 successful the first time
- The artifacts omit repo-level verification steps. Existing NUnit fixtures already initialize ParatextGlobals in a temp folder (c-sharp-tests/Projects/FixtureSetup.cs:17-35), and DummyScrText/LocalParatextProjects helpers live in `c-sharp-tests`, yet the workflow never instructs Phase 3 agents to base their tests on these utilities. This omission increases the odds of brittle, ad-hoc test harnesses.
- The workflow also does not mention how to run the relevant test subsets (`dotnet test c-sharp-tests/c-sharp-tests.csproj --filter Projects`, `pnpm test extensions/...`, etc.) or how to use the provided WEB sample project (LocalParatextProjects.cs:49-96) for manual validation. Without such guidance agents must rediscover the test infrastructure each time.
- Beyond “golden masters” and “property tests,” there is no encouragement to cross-check the new API via existing data providers (e.g., asserting that `ParatextProjectDataProviderFactory` sees the new project, or that commands surface through `papi-shared-types`). Those checks would give agents immediate feedback before human review.

## Recommendations & Next Steps
1. **Clarify golden master strategy for Level A**
   - Rename the Level A artifact to “behavior spec” or skip Golden Master Gen entirely when ParatextData is the oracle. If concrete samples are still desired, script PT10 integration tests that assert against ParatextData directly rather than maintaining prose JSON.
2. **Embed PT10 alignment in Phase 2 artifacts**
   - Add a required section to `data-contracts.md` that maps each API to an actual namespace (`Paranext.DataProvider.Services.ProjectCreationService`), file path (`c-sharp/Services/ProjectCreationService.cs`), and TypeScript command registration (`extensions/src/<ext>/src/main.ts`). This removes the need for a separate alignment agent.
3. **Make contracts match the repo from the start**
   - Replace `IProjectService` with concrete instructions to extend `LocalParatextProjects` or introduce a `ProjectCreationService` static helper so that code lands under `Paranext.DataProvider`. Align command names with the documented pattern (e.g., `platformProjects.createProject`), and extend `papi-shared-types` declarations accordingly.
4. **Reduce redundant artifacts**
   - Keep the detailed description in one canonical file (e.g., README) and let the GitHub issue/spec-summary link to that file rather than duplicating tables. Remove from phase-status any data that already exists elsewhere; use it only for gate tracking.
5. **Prioritize test assets over prose**
   - Instead of 75 JSON scenarios, capture the 8–10 inputs that actually become NUnit/Vitest cases and link each to the file where the test will live. Include commands for running those suites so agents know how to demonstrate G4–G8.
6. **Document verification hooks**
   - Add a short “Verification” section to the feature README that points to `c-sharp-tests/Projects/FixtureSetup.cs` for initializing ParatextGlobals, `DummyLocalParatextProjects` for in-memory projects, and the expected `dotnet test`/`pnpm test` invocations. Require agents to run these before handing work off.
7. **Capture PT10-specific requirements from the old repo**
   - Extend the boundary map with the PT10 classes that must change (e.g., `ParatextProjectDataProviderFactory`, `ProjectDataProvider.cs`, target extension manifests). Right now artifacts only cite PT9 files, so the PT10 work remains underspecified.
8. **Plan UI/command surfacing explicitly**
   - Decide which extension will expose the project-creation flow, list the files to edit, and capture any UX behavior that needs to be rebuilt. Without this, the agents will defer the UI plan indefinitely.
9. **Provide automated sanity checks**
   - Beyond the standard tests, add instructions to verify that `LocalParatextProjects.GetAllProjectDetails()` returns the new project and that `ScrTextCollection` contains the GUID after creation. These can be scripted as integration tests and give agents a concrete “done” signal.

Implementing the above will turn the existing documentation from an exhaustive PT9 catalog into a focused PT10 execution plan and should let future agents deliver code that matches repo conventions on their first attempt.
